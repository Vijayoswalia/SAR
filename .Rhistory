"# Overall correlation `Petal.Length` and `Petal.Width`\n",
"cor(iris$Petal.Length, iris$Petal.Width)\n",
"\n",
"# Return values of `iris` levels \n",
"x=levels(iris$Species)\n",
"\n",
"# Print Setosa correlation matrix\n",
"print(x[1])\n",
"cor(iris[iris$Species==x[1],1:4])\n",
"\n",
"# Print Versicolor correlation matrix\n",
"print(x[2])\n",
"cor(iris[iris$Species==x[2],1:4])\n",
"\n",
"# Print Virginica correlation matrix\n",
"print(x[3])\n",
"cor(iris[iris$Species==x[3],1:4])\n",
"\n",
"# Return all `iris` data\n",
"iris\n",
"\n",
"# Return first 5 lines of `iris`\n",
"head(iris)\n",
"\n",
"# Return structure of `iris`\n",
"str(iris)\n",
"\n",
"# Division of `Species`\n",
"table(iris$Species) \n",
"\n",
"# Percentual division of `Species`\n",
"round(prop.table(table(iris$Species)) * 100, digits = 1)\n",
"\n",
"# Summary overview of `iris`\n",
"summary(iris) \n",
"\n",
"# Refined summary overview\n",
"summary(iris[c(\"Petal.Width\", \"Sepal.Width\")])\n",
"\n",
"# Build your own `normalize()` function\n",
"normalize <- function(x) {\n",
"  num <- x - min(x)\n",
"  denom <- max(x) - min(x)\n",
"  return (num/denom)\n",
"}\n",
"\n",
"# Normalize the `iris` data\n",
"iris_norm <- as.data.frame(lapply(iris[1:4], normalize))\n",
"\n",
"# Summarize `iris_norm`\n",
"summary(iris_norm)\n",
"\n",
"set.seed(1234)\n",
"\n",
"ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))\n",
"\n",
"# Compose training set\n",
"iris.training <-iris[ind==1, 1:4]\n",
"\n",
"# Inspect training set\n",
"head(iris.training)\n",
"\n",
"# Compose test set\n",
"iris.test <-iris[ind==2, 1:4]\n",
"\n",
"# Inspect test set\n",
"head(iris.test)\n",
"\n",
"\n",
"# Compose `iris` training labels\n",
"iris.trainLabels <- iris[ind==1,5]\n",
"\n",
"# Inspect result\n",
"print(iris.trainLabels)\n",
"\n",
"# Compose `iris` test labels\n",
"iris.testLabels <- iris[ind==2, 5]\n",
"\n",
"# Inspect result\n",
"print(iris.testLabels)\n",
"\n",
"# Build the model\n",
"iris_pred <- knn(train = iris.training, test = iris.test, cl = iris.trainLabels, k=3)\n",
"\n",
"# Inspect `iris_pred`\n",
"print(iris_pred)\n",
"\n",
"# Put `iris.testLabels` in a data frame\n",
"irisTestLabels <- data.frame(iris.testLabels)\n",
"\n",
"# Merge `iris_pred` and `iris.testLabels` \n",
"merge <- data.frame(iris_pred, iris.testLabels)\n",
"\n",
"# Specify column names for `merge`\n",
"names(merge) <- c(\"Predicted Species\", \"Observed Species\")\n",
"\n",
"# Inspect `merge` \n",
"merge\n",
"\n",
"CrossTable(x = iris.testLabels, y = iris_pred, prop.chisq=FALSE)\n",
"\n",
"# Create index to split based on labels  \n",
"index <- createDataPartition(iris$Species, p=0.75, list=FALSE)\n",
"\n",
"# Subset training set with index\n",
"iris.training <- iris[index,]\n",
"\n",
"# Subset test set with index\n",
"iris.test <- iris[-index,]\n",
"\n",
"# Overview of algos supported by caret\n",
"names(getModelInfo())\n",
"\n",
"# Train a model\n",
"model_knn <- train(iris.training[, 1:4], iris.training[, 5], method='knn')\n",
"\n",
"# Predict the labels of the test set\n",
"predictions<-predict(object=model_knn,iris.test[,1:4])\n",
"\n",
"# Evaluate the predictions\n",
"table(predictions)\n",
"\n",
"# Confusion matrix \n",
"confusionMatrix(predictions,iris.test[,5])\n",
"\n",
"# Train the model with preprocessing\n",
"model_knn <- train(iris.training[, 1:4], iris.training[, 5], method='knn', preProcess=c(\"center\", \"scale\"))\n",
"\n",
"# Predict values\n",
"predictions<-predict.train(object=model_knn,iris.test[,1:4], type=\"raw\")\n",
"\n",
"# Confusion matrix\n",
"confusionMatrix(predictions,iris.test[,5])\n",
"\n"
]
}
],
"metadata": {
"_change_revision": 0,
"_is_fork": false,
"kernelspec": {
"display_name": "R",
"language": "R",
"name": "ir"
},
"language_info": {
"codemirror_mode": "r",
"file_extension": ".r",
"mimetype": "text/x-r-source",
"name": "R",
"pygments_lexer": "r",
"version": "3.3.3"
}
},
"nbformat": 4,
"nbformat_minor": 0
}
library(ggplot2) # Data visualization
library(ggvis)
library(gmodels)
library(class)
install.packages("ggvis")
library(ggplot2) # Data visualization
library(ggvis)
library(gmodels)
library(class)
library(caret)
library(lattice)
library(e1071)
iris
head(iris)
names(iris) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")
iris
iris %>% ggvis(~Sepal.Length, ~Sepal.Width, fill = ~Species) %>% layer_points()
iris %>% ggvis(~Petal.Length, ~Petal.Width, fill = ~Species) %>% layer_points()
cor(iris$Petal.Length, iris$Petal.Width)
x=levels(iris$Species)
print(x[1])
cor(iris[iris$Species==x[1],1:4])
print(x[2])
cor(iris[iris$Species==x[2],1:4])
print(x[3])
cor(iris[iris$Species==x[3],1:4])
iris
head(iris)
str(iris)
table(iris$Species)
round(prop.table(table(iris$Species)) * 100, digits = 1)
summary(iris)
summary(iris[c("Petal.Width", "Sepal.Width")])
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
}
summary(iris_norm)
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))
head(iris.training)
iris.test <-iris[ind==2, 1:4]
head(iris.test)
iris.trainLabels <- iris[ind==1,5]
print(iris.trainLabels)
iris.testLabels <- iris[ind==2, 5]
print(iris.testLabels)
iris.training <-iris[ind==1, 1:4]
set.seed(1234)
iris_norm <- as.data.frame(lapply(iris[1:4], normalize))
iris_pred <- knn(train = iris.training, test = iris.test, cl = iris.trainLabels, k=3)
print(iris_pred)
merge <- data.frame(iris_pred, iris.testLabels)
names(merge) <- c("Predicted Species", "Observed Species")
merge
CrossTable(x = iris.testLabels, y = iris_pred, prop.chisq=FALSE)
iris.training <- iris[index,]
names(getModelInfo())
predictions<-predict(object=model_knn,iris.test[,1:4])
table(predictions)
model_knn <- train(iris.training[, 1:4], iris.training[, 5], method='knn', preProcess=c("center", "scale"))
predictions<-predict.train(object=model_knn,iris.test[,1:4], type="raw")
confusionMatrix(predictions,iris.test[,5])
irisTestLabels <- data.frame(iris.testLabels)
confusionMatrix(predictions,iris.test[,5])
iris.test <- iris[-index,]
index <- createDataPartition(iris$Species, p=0.75, list=FALSE)
model_knn <- train(iris.training[, 1:4], iris.training[, 5], method='knn')
iris
head(iris)
names(iris) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")
iris
head(iris)
?%>%
iris %>% ggvis(~Sepal.Length, ~Sepal.Width, fill = ~Species) %>% layer_points()
iris %>% ggvis(~Sepal.Length, ~Sepal.Width, fill = ~Species) %>% layer_points()
iris %>% ggvis(~Petal.Length, ~Petal.Width, fill = ~Species) %>% layer_points()
plot(iris)
cor(iris$Petal.Length, iris$Petal.Width)
x=levels(iris$Species)
x=levels(iris$Species)
x
table(iris)
summary(Iris)
summaryiris)
summary(ris)
summary(iris)
mode(iris)
str(iris)
str(iris$Species)
names(Filter(is.factor, iris))
iris
head(iris)
Gender = c("M","F")
F = c("M","F")
Gender = seq(repeat,c("F","M"),25)
iriss = iris
iriss
?repeat
?repeat
?repeat()
?seq
rm(list = ls)
library(MASS)
install.packages("ISLR")
library(ISLR)
names(Boston)
plot(medv~lstat, Boston)
50 = read.csv("50_Startups.csv")
50 = read.csv("50_Startups.csv", header = TRUE)
50 = read.table("50_Startups.csv", header = TRUE)
50 = read.csv("50_Startups.csv", header = TRUE)
library(readr)
X50_Startups <- read_csv("E:/Data Science/rWork/rProjects/SAR/50_Startups.csv")
View(X50_Startups)
50SU = read.csv("50_Startups.csv", header = TRUE)
Startup = read.csv("50_Startups.csv", header = TRUE)
dataset = read.csv("50_Startups.csv")
dataset
names(dataset)
library(caTools)
set.seed(123)
split = sample.split(dataset$Profit, SplitRatio = 0.8)
training_set  =subset(dataset, split ==TRUE)
test_set = subset(dataset, split ==FALSE)
split
training_set
test_set
library(readr)
X50_Startups <- read_csv("E:/Data Science/rWork/rProjects/SAR/50_Startups.csv")
View(X50_Startups)
View(training_set)
view(test_set)
test_set
View(training_set)
test_set
View(training_set)
view(test_set)
View(test_set)
regressor = lm(formula = Profit ~., data = training_set)
y_pred = predict(regressor, newdata = test_set)
test_set$Profit_Pred = y_pred
test_set$Pred_error = test_set$Profit - test_set$Profit_Pred
MAD = mean(abs(test_set$Pred_error))
MAD
MSE = mean(test_set$Pred_error^2)
MSE
RSME = sqrt(mean(test_set$Pred_error^2))
RSME
Pred_Err = function(test_data, prediction_model)
{
pred = predict(prediction_model, newdata=test_data)
RMSE = sqrt(mean((test_data$Profit-pred)^2))
MAD = mean(abs(test_data$Profit-pred))
return(c(MSE,MAD))
}
Pred_Err(test_data = test_set, regressor)
regressor = lm(formula = Profit ~R.D.Spend, data = training_set)
summary(regressor)
regressor = lm(formula = Profit ~R.D.Spend+MArketing.Spend, data = training_set)
regressor = lm(formula = Profit ~R.D.Spend+Marketing.Spend, data = training_set)
regressor
regressor = lm(formula = Profit ~R.D.Spend, data = training_set)
summary(regressor)
Pred_Err(test_set, prediction_model = regressor)
regressor1 = lm(formula = Profit ~R.D.Spend+Marketing.Spend, data = training_set)
regressor
regressor1
regressor = lm(formula = Profit ~R.D.Spend, data = training_set)
summary(regressor)
Pred_Err(test_set, prediction_model = regressor)
Pred_Err(test_set, prediction_model = regressor1)
summary(regressor)
summary(regressor1)
Pred_Err(test_set, prediction_model = regressor)
Pred_Err(test_set, prediction_model = regressor1)
read.csv("logistic_regression_SNA.csv")
read.csv("logistic_regression_SNA.csv")
dataset = read.csv("logistic_regression_SNA.csv")
dataset = read_file("logistic_regression_SNA.R")
dataset
View(dataset)
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
training_set[-3] = scale(training_set[-3])
training_set[-3]
test_set[-3] = scale(test_set[-3])
test_set[-3]
classifier = glm(formula = Purchased ~ .,
family = binomial,
data = training_set)
summary(classifier)
?scale
# Logistic Regression
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
?scale
# Feature Scaling
training_set[-3] = scale(training_set[-3])
training_set[-3]
test_set[-3] = scale(test_set[-3])
test_set[-3]
# Fitting Logistic Regression to the Training set
classifier = glm(formula = Purchased ~ .,
family = binomial,
data = training_set)
summary(classifier)
# Predicting the Test set results
prob_pred = predict(classifier, type = 'response', newdata = test_set[-3])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
# Visualising the Training set results
# install.packages("ElemStatLearn")
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
install.packages("ElemStatLearn")
# Logistic Regression
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
?scale
# Feature Scaling
training_set[-3] = scale(training_set[-3])
training_set[-3]
test_set[-3] = scale(test_set[-3])
test_set[-3]
# Fitting Logistic Regression to the Training set
classifier = glm(formula = Purchased ~ .,
family = binomial,
data = training_set)
summary(classifier)
# Predicting the Test set results
prob_pred = predict(classifier, type = 'response', newdata = test_set[-3])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
# Visualising the Training set results
# install.packages("ElemStatLearn")
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualising the Test set results
library(ElemStatLearn)
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
test_set = subset(dataset, split == FALSE)
?scale
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
test_set[-3]
classifier = glm(formula = Purchased ~ .,
family = binomial,
data = training_set)
summary(classifier)
prob_pred = predict(classifier, type = 'response', newdata = test_set[-3])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
cm = table(test_set[, 3], y_pred)
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
training_set[-3]
training_set = subset(dataset, split == TRUE)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
install.packages(c("ape", "bigQueryR", "checkmate", "data.table", "glue", "heatmaply", "kableExtra", "lazyeval", "lsmeans", "mice", "proxy", "quantreg", "robustbase", "withr"))
x=levels(iris$Species)
x
